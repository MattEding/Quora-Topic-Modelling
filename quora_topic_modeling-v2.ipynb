{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "import string\n",
    "\n",
    "import joblib\n",
    "import textblob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(style=\"whitegrid\")\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('quora_challenge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the vast majority of questions are short texts\n",
    "X['question_text'].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['question_len'] = X['question_text'].str.len()\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(20, 5))\n",
    "fig.suptitle('Question Length Distribution', fontsize=16)\n",
    "\n",
    "sns.violinplot(X['question_len'], inner='quartile', orient='h', ax=ax0)\n",
    "ax0.set_xlabel('Character Length')\n",
    "\n",
    "sns.violinplot(X['question_len'], inner='quartile', orient='h', ax=ax1)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Character Length (Log Scale)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, *, target_col='question_text', result_col='cleaned_text', pos='anvr', stop_words=None):\n",
    "        self.target_col = str(target_col)\n",
    "        self.result_col = str(result_col)\n",
    "        self.pos = tuple(pos.lower()) if isinstance(pos, str) else tuple(pos)\n",
    "        self.stop_words = () if stop_words is None else frozenset(stop_words)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X[self.result_col] = (X[self.target_col].str.lower()\n",
    "                                                .map(self.asciitize)\n",
    "                                                .map(self.depunctuate)\n",
    "                                                .map(self.lemmatize))\n",
    "        return X[self.result_col].values\n",
    "        \n",
    "    @staticmethod\n",
    "    def asciitize(text):\n",
    "        return ''.join(char for char in text if char in string.printable)\n",
    "\n",
    "    @staticmethod\n",
    "    def depunctuate(text):\n",
    "        return ''.join(char if char not in string.punctuation else ' ' for char in text)\n",
    "    \n",
    "    def lemmatize(self, text):\n",
    "        tag_dict = dict(J='a', N='n', V='v', R='r')\n",
    "        blob = textblob.TextBlob(text)\n",
    "        \n",
    "        try:\n",
    "            words, tags = zip(*blob.pos_tags)\n",
    "        except ValueError:\n",
    "            return ()\n",
    "        \n",
    "        tags = (tag_dict.get(tag[0]) for tag in tags)\n",
    "        lemmas = (word.lemmatize(tag) for word, tag in zip(words, tags)\n",
    "                  if tag in self.pos\n",
    "                  if word not in self.stop_words)\n",
    "        return ' '.join(lem for lem in lemmas if lem not in self.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRA_STOP_WORDS = frozenset(\"does doesn doesnt don dont im ive make quora really shouldnt youll ve weve wouldnt\".split())\n",
    "NOT_STOP_WORDS = frozenset(\"cry system\".split())\n",
    "CUSTOM_STOP_WORDS = ENGLISH_STOP_WORDS | EXTRA_STOP_WORDS - NOT_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to check if topic labelling seems to make sense later\n",
    "X_tr, X_te = train_test_split(X, test_size=0.001, random_state=0)\n",
    "len(X_tr), len(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_kwds = dict(\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words=None,\n",
    "    lowercase=False,\n",
    "    max_df=0.9,\n",
    "    max_features=5_000,\n",
    ")\n",
    "\n",
    "decomp_kwds = dict(\n",
    "    n_components=10,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "topic_model_pipe = Pipeline([\n",
    "    ('textprep', TextPreprocessor()),\n",
    "    ('vectorizer', TfidfVectorizer(**vector_kwds)),\n",
    "    ('decomposer', NMF(**decomp_kwds))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME\n",
    "topic_model_pipe.fit(X_te)\n",
    "file_prefix = '_'.join(type(step).__name__ for step in topic_model_pipe.named_steps.values()).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(topic_model_pipe, f'{file_prefix}__pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words for each category -- does not need X\n",
    "n_top = 8\n",
    "\n",
    "feature_names = topic_model_pipe['vectorizer'].get_feature_names()\n",
    "pyfunc = functools.partial(operator.getitem, feature_names)\n",
    "vfunc = np.vectorize(pyfunc)\n",
    "\n",
    "components = topic_model_pipe['decomposer'].components_\n",
    "word_idxs = components.argsort(axis=1)[:, -n_top:][:, ::-1]\n",
    "words = vfunc(word_idxs)\n",
    "\n",
    "top_words_df = pd.DataFrame(words.T, columns=[f'Topic {i+1:0>2}' for i in range(len(words))])\n",
    "top_words_df.index.name = 'Top Words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(top_words_df, f'{file_prefix}__top_words.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = topic_model_pipe.transform(X_te) # <-- FIXME X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic count distribution\n",
    "topics = np.argmax(X_new, axis=1)\n",
    "x, y = np.vstack(np.unique(topics, return_counts=True))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x, y, palette='GnBu_d')\n",
    "plt.title('Topic Distribution')\n",
    "plt.xticks(ticks=range(len(x)), labels=list('abcdefghijk')) # <-- FIXME labels\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top representative sentences\n",
    "n_sents = 5\n",
    "repr_idxs = np.argsort(X_new, axis=0)[-n_sents:].T\n",
    "repr_sents = X['question_text'].values[repr_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(repr_sents, f'{file_prefix}__repr_sents.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME X\n",
    "\n",
    "# Label sentences to their best topic\n",
    "X_tr['topic'] = np.argmax(X_new, axis=1)\n",
    "\n",
    "joblib.dump(X_tr, f'{file_prefix}__X_tr_clean_topic.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
