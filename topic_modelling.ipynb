{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "import string\n",
    "\n",
    "import joblib\n",
    "import textblob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('quora_challenge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['question_text'].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['question_len'] = X['question_text'].str.len()\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(20, 5))\n",
    "fig.suptitle('Question Length Distribution', fontsize=16)\n",
    "\n",
    "sns.violinplot(X['question_len'], inner='quartile', orient='h', ax=ax0)\n",
    "ax0.set_xlabel('Character Length')\n",
    "\n",
    "sns.violinplot(X['question_len'], inner='quartile', orient='h', ax=ax1)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Character Length (Log Scale)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, *, target_col='question_text', result_col='cleaned_text', pos='anvr', stop_words=None):\n",
    "        self.target_col = str(target_col)\n",
    "        self.result_col = str(result_col)\n",
    "        self.pos = tuple(pos.lower()) if isinstance(pos, str) else tuple(pos)\n",
    "        self.stop_words = () if stop_words is None else frozenset(stop_words)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X[self.result_col] = (X[self.target_col].str.lower()\n",
    "                                                .map(self.asciitize)\n",
    "                                                .map(self.depunctuate)\n",
    "                                                .map(self.lemmatize))\n",
    "        return X[self.result_col].values\n",
    "        \n",
    "    @staticmethod\n",
    "    def asciitize(text):\n",
    "        return ''.join(char for char in text if char in string.printable)\n",
    "\n",
    "    @staticmethod\n",
    "    def depunctuate(text):\n",
    "        return ''.join(char if char not in string.punctuation else ' ' for char in text)\n",
    "    \n",
    "    def lemmatize(self, text):\n",
    "        tag_dict = dict(J='a', N='n', V='v', R='r')\n",
    "        blob = textblob.TextBlob(text)\n",
    "        \n",
    "        try:\n",
    "            words, tags = zip(*blob.pos_tags)\n",
    "        except ValueError:\n",
    "            return ''\n",
    "        \n",
    "        tags = (tag_dict.get(tag[0]) for tag in tags)\n",
    "        lemmas = (word.lemmatize(tag) for word, tag in zip(words, tags)\n",
    "                  if tag in self.pos\n",
    "                  if word not in self.stop_words)\n",
    "        result = ' '.join(lem for lem in lemmas if lem not in self.stop_words)\n",
    "        return result if result else ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicLabeller(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        topics = np.argmax(X, axis=1)\n",
    "        weights = np.max(X, axis=1)\n",
    "        return np.vstack([topics, weights]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRA_STOP_WORDS = frozenset(\"does doesn doesnt don dont im ive make quora really shouldnt youll ve weve wouldnt\".split())\n",
    "NOT_STOP_WORDS = frozenset(\"cry system\".split())\n",
    "CUSTOM_STOP_WORDS = ENGLISH_STOP_WORDS | EXTRA_STOP_WORDS - NOT_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_kwds = dict(\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words=None,\n",
    "    lowercase=False,\n",
    "    max_df=0.9,\n",
    "    max_features=25_000,\n",
    ")\n",
    "\n",
    "decomp_kwds = dict(\n",
    "    n_components=50,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "topic_model_pipe = Pipeline([\n",
    "    ('textprep', TextPreprocessor()),\n",
    "    ('vectorizer', TfidfVectorizer(**vector_kwds)),\n",
    "    ('decomposer', NMF(**decomp_kwds)),\n",
    "    ('labeller', TopicLabeller()),\n",
    "], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = topic_model_pipe.fit_transform(X)\n",
    "\n",
    "file_prefix = ('_'.join(type(step).__name__ for step in topic_model_pipe.named_steps.values()).lower()\n",
    "               + f'_topics{topic_model_pipe[\"decomposer\"].n_components}'\n",
    "               + f'_mxfeat{topic_model_pipe[\"vectorizer\"].max_features}'\n",
    "              )\n",
    "print(file_prefix)\n",
    "\n",
    "# joblib.dump(topic_model_pipe, f'{file_prefix}__pipeline.joblib')\n",
    "# joblib.dump(X_new, f'{file_prefix}__X_new.joblib');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['topic'] = X_new[:, 0]\n",
    "X['weight'] = X_new[:, 1]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_words(pipeline, n_top=8):\n",
    "    feature_names = pipeline['vectorizer'].get_feature_names()\n",
    "    pyfunc = functools.partial(operator.getitem, feature_names)\n",
    "    vfunc = np.vectorize(pyfunc)\n",
    "\n",
    "    components = pipeline['decomposer'].components_\n",
    "    word_idxs = components.argsort(axis=1)[:, -n_top:][:, ::-1]\n",
    "    words = vfunc(word_idxs)\n",
    "\n",
    "    top_words_df = pd.DataFrame(words.T, columns=[f'Topic {i}' for i in range(len(words))])\n",
    "    top_words_df.index.name = 'Top Words'\n",
    "    \n",
    "    return top_words_df\n",
    "\n",
    "top_words_df = extract_top_words(topic_model_pipe)\n",
    "top_words_df\n",
    "\n",
    "# joblib.dump(top_words_df, f'{file_prefix}__top_words.joblib')\n",
    "# top_words_df.to_csv(f'file_prefix}__top_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic count distribution\n",
    "topics = X['topic']\n",
    "x, y = np.vstack(np.unique(topics, return_counts=True))\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.barplot(x, y, palette='GnBu_d')\n",
    "plt.title('Topic Distribution')\n",
    "plt.xticks(ticks=range(len(x)))\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# plt.savefig(f'{file_prefix}__topic_dist.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = np.ceil(X['weight'].max() * 100) / 100\n",
    "xs = np.linspace(0, stop, 1000)\n",
    "ys = np.empty_like(xs)\n",
    "for i, x in enumerate(xs):\n",
    "    ys[i] = np.sum(X['weight'] > x) / len(X)\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(20, 5))\n",
    "fig.suptitle('Ratio of Question Weights Above Threshold', fontsize=16)\n",
    "\n",
    "ax0.plot(xs, ys)\n",
    "ax0.set_xlabel('Weight Threshold')\n",
    "ax0.set_ylabel('Ratio')\n",
    "\n",
    "ax1.plot(xs, ys)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Weight Threshold (Log Scale)')\n",
    "ax1.set_ylabel('Ratio')\n",
    "\n",
    "# plt.savefig(f'{file_prefix}__thresh_ratio.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_sentences(n_sents=3):\n",
    "    for group, df in X.groupby('topic'):\n",
    "        print('Topic', group)\n",
    "\n",
    "        repr_idxs = df['weight'].argsort()[-n_sents:].values\n",
    "        repr_wghts = df['weight'].sort_values()[-n_sents:].values.round(4)    \n",
    "        repr_sents = df['question_text'].values[repr_idxs]\n",
    "\n",
    "        for sent, wght in zip(repr_sents, repr_wghts):\n",
    "            print('\\t', wght, sent)\n",
    "\n",
    "        print('-' * 100)\n",
    "        \n",
    "representative_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sentences(n_sents=3, random_state=None):\n",
    "    for group, df in X.groupby('topic'):\n",
    "        print('Topic', group)\n",
    "        \n",
    "        sample = df.sample(n_sents, random_state=random_state)\n",
    "        rand_sents = sample['question_text'].values\n",
    "        rand_wghts = sample['weight'].values.round(4)\n",
    "        \n",
    "        for sent, wght in zip(rand_sents, rand_wghts):\n",
    "            print('\\t', wght, sent)\n",
    "\n",
    "        print('-' * 100)\n",
    "        \n",
    "random_sentences(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
